<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Daniel Rodriguez">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>Daniel Rodriguez - Articles by Daniel Rodriguez | Daniel Rodriguez</title>

        <link rel="alternate" type="application/atom+xml" title="Daniel Rodriguez blog atom feed" href="/feeds/all.atom.xml" />
        <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
        <link rel="stylesheet" href="/theme/css/pygments.css">
        <link rel="stylesheet" href="/theme/css/main.css">

    </head>

    <body>
        <header class="navbar navbar-static-top bs-docs-nav main-header">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand" href="/" title="Home" class="title">Daniel Rodriguez</a></li>
                </div>
                <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
                    <ul class="nav navbar-nav navbar-right">
                            <li><a href="/pages/about.html" title="About">About</a></li>
                            <li><a href="http://github.com/danielfrg" title="danielfrg's Github" rel="me">Github</a></li>
                            <li><a href="http://twitter.com/danielfrg" title="danielfrg Twitter" rel="me">Twitter</a></li>
                            <li><a href="/archives.html" title="Archive">Archive</a></li>
                            <li><a href="/feeds/all.atom.xml" title="danielfrg.github.io RSS feed" rel="me">RSS</a></li>
                    </ul>
                </nav>
            </div>
        </header>

<div class="container index">

  <article>
    <header>
      <h2><a href="/blog/2015/04/30/reproduce-it/">ReproduceIt</a></h2>
      <time datetime="" title="2015-04-30T00:00:00+00:00" pubdate>April 30, 2015</time>
    </header>

    <div class="article_content">
      <p>I have been wanting to restart my blog for a while now. In 2013 I wrote around 20 posts but in 2014 I only wrote 3 times and this is my first post in 2015 and its not even a real one. Last one was more than 6 months ago. 
Time is one of the reasons but sometimes I have a couple of hours to kill that I would like to use to blog but ideas don't come to me easily sometimes.</p>
<p>Last weekend I attended <a href="http://pydata.org/dal2015">PyData Dallas 2015</a> and in some of the talks about data journalism and open data I got a simple but I believe effective idea for getting me writing new posts.</p>
<p>The idea is to take articles from the internet that do some kind of data analysis and reproduce the results. In most of the cases the data and analysis are not published but ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2015/04/30/reproduce-it/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/reproduceit.html" class="tag">ReproduceIt</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2014/08/01/storm-sklearn/">From zero to storm cluster for scikit-learn classification</a></h2>
      <time datetime="" title="2014-08-01T00:00:00+00:00" pubdate>August 01, 2014</time>
    </header>

    <div class="article_content">
      <p><a href="http://https://storm.incubator.apache.org/">Apache storm</a> is a new technology
that allows to do real-time computation that its been in the big data news
lately and I was curious to try it to see if its really good or is a just
the new map-reduce.</p>
<p>One of the first (and no brainer) ideas I had was to do real-time
classification of a scikit-learn model, the main issue was that storm is Java and
I didn't want to all the integration between Java and Python
but after I saw in the pydata videos that the people at
<a href="https://parsely.com/">Parsely</a> already took some of that pain out with
their new <a href="https://github.com/Parsely/streamparse">streamparse</a> library
I had no more excuses to try it.</p>
<h2>Storm cluster</h2>
<p>I decided to deploy a storm cluster and after failing to use their EC2 scripts
I decided to do it myself using <a href="http://saltstack.com">salt</a>.
I found an amazing <a href="http://www.michael-noll.com/tutorials/running-multi-node-storm-cluster">step by step tutorial</a>
from  Michael Noll ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2014/08/01/storm-sklearn/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/storm.html" class="tag">storm</a>
            <a href="/tag/salt.html" class="tag">salt</a>
            <a href="/tag/sklearn.html" class="tag">sklearn</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/11/27/ipython-notebook-aws-salt/">One-liner: Deploy python scipy stack with IPython notebook on AWS</a></h2>
      <time datetime="" title="2013-11-27T00:00:00+00:00" pubdate>November 27, 2013</time>
    </header>

    <div class="article_content">
      <p><strong>Problem:</strong> How many times have you needed to create a powerful EC2 instance with the the python scientific stack
installed and the ipython notebook running? I have to do this at least 2 or 3 times every week.</p>
<p>A simple solution is to have an AMI with all the libraries ready and create a new instance every time,
you can even have the ipython notebook as an upstart service in ubuntu so it runs when the instance is ready.
That was my previous solution, but that was before I learned <a href="http://saltstack.com/">salt</a>.</p>
<p>The problem with the AMI solution is that it gets dated really quickly and while update it is not hard, it is
annoying. Also having to login into AWS, look for the AMI and spin up and instance is annoying.</p>
<p><strong>Solution:</strong> <a href="http://saltstack.com/">Salt</a> + <a href="http://continuum.io/downloads">Anaconda</a> + <a href="http://www.vagrantup.com/">Vagrant</a></p>
<p>Salt will do the provisioning of the instance using states, that makes the updates of ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/11/27/ipython-notebook-aws-salt/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/aws.html" class="tag">aws</a>
            <a href="/tag/salt.html" class="tag">salt</a>
            <a href="/tag/ipython-notebook.html" class="tag">ipython-notebook</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/11/23/harvard-ds/">Review: Harvard Data Science - Fall 2013</a></h2>
      <time datetime="" title="2013-11-23T00:00:00+00:00" pubdate>November 23, 2013</time>
    </header>

    <div class="article_content">
      <p>A few weeks/months ago I found on HackerNews that Harvard was publishing all their <a href="http://cs109.org/">Data Science course content</a> (lectures, videos, labs, homeworks) online. I couldn't miss the opportunity to find what are they teaching, it was a nice experience to see how much I have learned and what else I am missing. Also, when I saw that everything was on python I knew they are doing it right!</p>
<p>The homeworks are IPython notebooks, the books are what I consider every Data Scientist should read: <a href="http://shop.oreilly.com/product/0636920023784.do">Python for Data Analysis</a>, <a href="http://shop.oreilly.com/product/0636920018483.do">Machine Learning for Hackers</a>, <a href="http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Prologue/Prologue.ipynb">Probabilistic Programming and Bayesian methods for Hackers</a>.</p>
<p>The homeworks were amazing and useful, I haven't seen all the videos (yet) but the ones I did were amazing. Below I do a little review of each homework, the stuff I learned and stuff that I think should be different.</p>
<p>Initially I wanted to use this opportunity ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/11/23/harvard-ds/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/data-science.html" class="tag">data science</a>
            <a href="/tag/nlp.html" class="tag">NLP</a>
            <a href="/tag/statistics.html" class="tag">statistics</a>
            <a href="/tag/bayesian.html" class="tag">Bayesian</a>
            <a href="/tag/graphs.html" class="tag">graphs</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/11/17/nlp-scale-semafor-salt-celery-more/">NLP at scale: Semafor + salt + celery and more</a></h2>
      <time datetime="" title="2013-11-17T00:00:00+00:00" pubdate>November 17, 2013</time>
    </header>

    <div class="article_content">
      <p>This posts describes the implementation of a simple system to parse web pages using <a href="http://www.ark.cs.cmu.edu/SEMAFOR/">SEMAFOR</a> (a SEMantic Analyzer Of Frame Representations) at scale. The system is mainly powered by salt and celery but also uses boto to create worker EC2 instances that parse the documents in parallel and luigi is used to describe the data pipeline in each worker.</p>
<p>The whole source can be found on github: <a href="https://github.com/danielfrg/semafor-parsing">danielfrg/semafor-parsing</a></p>
<p>The main idea is the following:</p>
<ol>
<li>We are going to have one master box that has: salt master + celery worker that is going to be waiting for tasks</li>
<li>When the master receives a query (list of urls to parse) is going to spin up N number of minions/workers using boto and is going to provision all of them using salt</li>
<li>Each minion/worker is going to have SEMAFOR and a celery worker waiting for parsing tasks</li>
<li>The master creates a ...</li></ol>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/11/17/nlp-scale-semafor-salt-celery-more/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/nlp.html" class="tag">NLP</a>
            <a href="/tag/semafor.html" class="tag">semafor</a>
            <a href="/tag/salt.html" class="tag">salt</a>
            <a href="/tag/celery.html" class="tag">celery</a>
            <a href="/tag/luigi.html" class="tag">luigi</a>
            <a href="/tag/vagrant.html" class="tag">vagrant</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/10/31/month-log/">Month-Log.oct.2013</a></h2>
      <time datetime="" title="2013-10-31T00:00:00+00:00" pubdate>October 31, 2013</time>
    </header>

    <div class="article_content">
      <p>I've been busy with real work to write any specific posts for the blog but I realize it was
a productive month. I worked a lot and learned quite a few things and  new technologies and
I have some thoughts about them, I usually use twitter to share simple thoughts but they usually get
lost on the noise.</p>
<p>So I am going to start a monthly series in which I discuss a little bit about what I have done that month.
Hopefully mixing with regular posts I am hoping that this makes me learn more and more stuff every month so I have something to write about. On this case is October and a few weeks of September.</p>
<h2>Books I read</h2>
<p>I had to do some <a href="http://pig.apache.org/">pig</a> for my job so I used this opportunity to
consolidate a little bit my knowledge.
I had worked with pig a little ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/10/31/month-log/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/month-log.html" class="tag">month-log</a>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/pig.html" class="tag">pig</a>
            <a href="/tag/nutch.html" class="tag">nutch</a>
            <a href="/tag/crawling.html" class="tag">crawling</a>
            <a href="/tag/vagrant.html" class="tag">vagrant</a>
            <a href="/tag/salt.html" class="tag">salt</a>
            <a href="/tag/luigi.html" class="tag">luigi</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/09/21/word2vec-yhat/">word2vec in yhat: Word vector similarity</a></h2>
      <time datetime="" title="2013-09-21T00:00:00+00:00" pubdate>September 21, 2013</time>
    </header>

    <div class="article_content">
      <p>A <a href="http://google-opensource.blogspot.com/2013/08/learning-meaning-behind-words.html">few weeks ago</a>
Google released some code to convert words to vectors called
<a href="https://code.google.com/p/word2vec/">word2vec</a>.
The company I am currently working on does something similar and I was quite amazed by the performance
and accuracy of Google's algorithm so I created a simple python wrapper to call the C code for training
and read the training vectors into numpy arrays, you can check it out on
<a href="https://pypi.python.org/pypi/word2vec">pypi (word2vec)</a>.</p>
<p>At the same time I found out about <a href="http://yhathq.com/">yhat</a>, I found about them
via twitter and after reading their blog I had to try their product. What they do is very simple
but very useful: take some python (scikit-learn) or R classifier and create a REST
endpoint to make predictions on new data. The product is still in very beta but the guys were
very responsive and they helped me to solve some of my issues.</p>
<p>The only restriction I had ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/09/21/word2vec-yhat/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/word2vec.html" class="tag">word2vec</a>
            <a href="/tag/yhat.html" class="tag">yhat</a>
            <a href="/tag/machine-learning.html" class="tag">machine learning</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/09/11/django-celery-readability-crawler/">Django + Celery + Readability = Python relevant content crawler</a></h2>
      <time datetime="" title="2013-09-11T00:00:00+00:00" pubdate>September 11, 2013</time>
    </header>

    <div class="article_content">
      <p>I have written a <a href="{filename}../08/blog-crawler.ipynb">few</a> <a href="{filename}../04/nba-data.md">posts</a>
about crawling content from the web. Mainly because I know the power of data
and the biggest data source in the world is the web, Google knew it and we all now how they are
doing. In my <a href="{filename}../08/blog-crawler.ipynb">last post</a> I wrote about crawling relevant content
from blogs; It worked but what if I want an admin UI to control the crawling. What if I just put
the crawler in an EC2 instance and just call it when I need to crawl.</p>
<p>The solution was pretty simple thanks to some python projects.
I just needed to move from <a href="http://www.sqlalchemy.org/">sqlalchemy</a> to
<a href="https://docs.djangoproject.com/en/1.5/">django</a> ORM, create a few <a href="http://celeryproject.org/">celery</a> tasks
and use <a href="https://github.com/celery/django-celery/">django-celery</a> to have a pretty UI of the tasks.</p>
<p>I was amazed on how easy it was to integrate celery with django. I just created a few tasks to
actually crawl blogs (I already had ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/09/11/django-celery-readability-crawler/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/django.html" class="tag">django</a>
            <a href="/tag/celery.html" class="tag">celery</a>
            <a href="/tag/readability.html" class="tag">readability</a>
            <a href="/tag/crawling.html" class="tag">crawling</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/04/01/nba-scraping-data/">Extracting NBA data from ESPN</a></h2>
      <time datetime="" title="2013-04-01T00:00:00+00:00" pubdate>April 01, 2013</time>
    </header>

    <div class="article_content">
      <p>I've been wanting to play with some sports data for a while. Today I decide to
stop procastinating and do it. The problems was that after searching a
while (15 minutes) for some data I was unable to find the data I wanted.
Even in the <a href="http://www.databasebasketball.com/">Basktetball Database</a>
(not really sure I undestand the site).</p>
<p>A friend showed me the <a href="http://espn.go.com/nba/">ESPN</a> stats and ask me if I
knew how to scrap the data from a website. I lied and told him Yes.
But I know python and its magic powers so after reading 15 minutes I knew how to do it.</p>
<p>I used <a href="http://docs.python-requests.org/en/latest/">requests</a> and
<a href="http://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a> to download and
scrap the data from the ESPN site. Then used <a href="http://pandas.pydata.org/">pandas</a>
to order, slice, and save the data into simple csv files. Also used
<a href="http://ipython.org/">iPython notebooks</a> to develop the code faster.
And a little bit of my <a href="https://github.com/danielfrg/copper">copper</a>
project to use ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/04/01/nba-scraping-data/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/crawling.html" class="tag">crawling</a>
            <a href="/tag/pandas.html" class="tag">pandas</a>
            <a href="/tag/requests.html" class="tag">requests</a>
            <a href="/tag/beautifulsoup.html" class="tag">beautifulsoup</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>
  <article>
    <header>
      <h2><a href="/blog/2013/03/08/pelican-ipython-notebook-plugin/">Plugin for blogging with IPython notebooks in Pelican</a></h2>
      <time datetime="" title="2013-03-08T00:00:00+00:00" pubdate>March 08, 2013</time>
    </header>

    <div class="article_content">
      <p>This is just a little update on my previous post about <a href="/blog/2013/02/16/blogging-pelican-ipython-notebook/">Blogging with iPython notebooks with pelican</a>.</p>
<p>One of the people behind pelican helped me to convert the <a href="/blog/2013/02/16/blogging-pelican-ipython-notebook/">previous code</a> to a pelican plugin and I just made it available via github: <a href="https://github.com/danielfrg/pelican-ipythonnb">pelican-ipythonnb</a>.</p>
<p>The only thing that changed is the installation but the docs on how to use it is below (or in the readme of the repo).</p>
<p>An example is my last post about a <a href="{filename}kaggle-bulldozers-clean.ipynb">cleaning data for a Kaggle competition</a>.</p>
<p>Happy blogging!</p>
<h2>Installation</h2>
<p>Download plugin files: <code>plugin/ipythonnb.py</code> and the <code>plugin/nbconverter</code> directory.</p>
<p>The esiest way is to locate the pelican directory (for example: <code>~/.virtualenvs/blog/lib/python2.7/site-packages/pelican/</code>) and paste plugins files in the <code>pelican/plugins</code> folder.
Then in the <code>pelicanconf.py</code> put: <code>PLUGINS = ['pelican.plugins.ipythonnb']</code>.</p>
<p>But is also is possible to add plugins on the same directory of the pelican project:
Create ...</p>
    </div>

    <div class="meta">
      <div>
        <a href="/blog/2013/03/08/pelican-ipython-notebook-plugin/" class="read_more">Read more &#8594;</a>
      </div>

      <div>
            <a href="/tag/python.html" class="tag">python</a>
            <a href="/tag/pelican.html" class="tag">pelican</a>
      </div>

    </div>

  </article>
  <div class="separator"></div>

  <nav class="pagination row">
    <div class="col-xs-6 left">
        <a class="prev" href="/author/daniel-rodriguez2.html">&#8592; Past</a>
    </div>
    <div class="col-xs-6 right">
    </div>
  </nav>

  </div>

</div>

        <style type="text/css">.text_cell .prompt {
    display: none;
}

div.cell {
    padding: 0;
}

div.text_cell_render {
    padding: 0;
}

div.prompt {
    font-size: 13px;
}

div.input_prompt {
    padding: .7em 0.2em;
}

div.output_prompt {
    padding: .4em .2em;
}

div.input_area {
    margin: .2em 0.4em;
}

table.dataframe {
    font-family: Arial, sans-serif;
    font-size: 13px;
    line-height: 20px;
}

table.dataframe th, td {
    padding: 4px;
    text-align: left;
}</style>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35523657-2', 'danielfrg.com');
  ga('send', 'pageview');

</script>

    </body>
</html>