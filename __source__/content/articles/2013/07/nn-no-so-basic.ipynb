{
 "metadata": {
  "name": "v3_1-blog"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My [previous post](http://danielfrg.github.io/blog/2013/07/03/basic-neural-network-python/) on implementing a basic Neural Network on python got a lot of attention staying one whole day on HN front page. I was very happy about that but more about the [feedback](https://news.ycombinator.com/item?id=5994851) I got. The community gave me a lot of tips and tricks on how to improve. So now I am presenting an improved version which supports multiple hidden layers and some optimization options. Maybe I can get some page-views by using the same topic :P"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I kept reading a lot about Neural Networks mainly by watching some videos from [Geoffrey's Hinton Neural Network course](https://class.coursera.org/neuralnets-2012-001/class/index) on coursera. Also reading more on [deeplearning.net](http://deeplearning.net/tutorial/) and trying to read some papers. That last task was definitely the hardest one because of the complexity of the papers. If I cannot event read them I can just imagine how hard is to write them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "About the Neural networks course I did not like it as much as the Machine Learning course. The main reason is that I had to watch most videos 3 or 4 times before understanding (something). This is definitely my fault because the material is great but it focuses more on the theory (math) which I am not that good at and not on the implementation which I am more interested. But I take it as a learning experience and I will try to finish those videos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some people criticized (besides my orthography, sorry about my spanglish :P) that I did not used cross-validation or more complex dataset on my previous post. I agree that both things are needed but I am not trying to make this post as a scientific paper with complete benchmarks of different optimization algorithms I leave that to the researchers who have spent years studying neural networks.\n",
      "\n",
      "My objective was to learn about them and to have some personal implementation of the algorithm, just to say that I have done it :P; but in on this case I am using the MNIST dataset to compare."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I was very tempted to use [theano](http://deeplearning.net/software/theano/) and but at the end I decide to keep using a pure numpy implementation.\n",
      "\n",
      "It is defenitly possible (and not that hard neither that easy) to create a theano implementation and I like some of the ideas such as creating a class for each layer because it makes easier to combine and create different implementations. But besides that I still cannot see the value of using theano. Of course the speed up is important but I thing I prefer the [Numba](http://numba.pydata.org/) approach."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation is very similar to the previous post, the differences are:\n",
      "\n",
      "1. Supports multiple hidden layers is using a similar implementation as the previous post: changed the gradient calculation to a for loop.\n",
      "2. Support for Minibach optimization, which is definitely a must have with bigger datasets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import math\n",
      "import numpy as np\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_network(layers):\n",
      "    return [(layers[i + 1], layers[i] + 1) for i in range(len(layers) - 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pack_weigths(*weights):\n",
      "    tuples = tuple([theta.reshape(-1) for theta in weights])\n",
      "    return np.concatenate(tuples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_gen(weights, weights_meta):\n",
      "    start_pos = 0\n",
      "    for layer in weights_meta:\n",
      "        end_pos = start_pos + layer[0] * (layer[1])\n",
      "        theta = weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        yield theta\n",
      "        start_pos = end_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_gen_inv(weights, weights_meta):\n",
      "    end_pos = len(weights)\n",
      "    for layer in reversed(weights_meta):\n",
      "        start_pos = end_pos - layer[0] * (layer[1])\n",
      "        theta = weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        yield theta\n",
      "        end_pos = start_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rand_init(weights_meta, epsilon_init):\n",
      "    s = 0\n",
      "    for t in weights_meta:\n",
      "        s += t[0] * t[1]\n",
      "    return np.random.rand(s, ) * 2 * epsilon_init - epsilon_init"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return np.divide(1, (1 + np.exp(-z)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward(weights, weights_meta, X, act_func):\n",
      "    m = X.shape[0]\n",
      "    ones = np.ones(m).reshape(m, 1)\n",
      "            \n",
      "    a_prev = np.hstack((ones, X))  # Input layer\n",
      "    for theta in unpack_weigths_gen(weights, weights_meta):\n",
      "        # Hidden Layers\n",
      "        z = np.dot(theta, a_prev.T)\n",
      "        a = act_func(z)\n",
      "        a_prev = np.hstack((ones, a.T))\n",
      "    return a  # Output layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sumsqr(a):\n",
      "    return np.sum(a ** 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def function(weights, X, y, weights_meta, num_labels, reg_lambda, act_func):\n",
      "    m = X.shape[0]\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    \n",
      "    h = forward(weights, weights_meta, X, act_func)\n",
      "    costPositive = -Y * np.log(h).T\n",
      "    costNegative = (1 - Y) * np.log(1 - h).T\n",
      "    cost = costPositive - costNegative\n",
      "    J = np.sum(cost) / m\n",
      "    \n",
      "    if reg_lambda != 0:\n",
      "        sums_qr = 0\n",
      "        for theta in unpack_weigths_gen(weights, weights_meta):\n",
      "            theta_filtered = theta[:, 1:]\n",
      "            sums_qr += sumsqr(theta_filtered)\n",
      "        reg = (reg_lambda / (2 * m)) * (sums_qr)\n",
      "        J = J + reg\n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def function_prime(weights, X, y, weights_meta, num_labels, reg_lambda, act_func):\n",
      "    m = X.shape[0]\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    ones = np.array(1).reshape(1,)\n",
      "    \n",
      "    d_s = ()\n",
      "    Deltas = [np.zeros(w) for i, w in enumerate(weights_meta)]\n",
      "    for i, row in enumerate(X):\n",
      "        # Forward\n",
      "        a_prev = np.hstack((ones, row))  # Input layer\n",
      "        a_s = (a_prev, )  # a_s[0] == a1\n",
      "        for j, theta in enumerate(unpack_weigths_gen(weights, weights_meta)):\n",
      "            # Hidden Layers\n",
      "            z = np.dot(theta, a_prev.T)\n",
      "            a = act_func(z)\n",
      "            a_prev = np.hstack((ones, a.T))\n",
      "            a_s = a_s + (a_prev, )\n",
      "                \n",
      "        # Backprop\n",
      "        d_prev = a_s[-1][1:] - Y[i, :].T  # last d\n",
      "        d_s = (d_prev, )  # d_s[0] == d2 \n",
      "        for a_i, theta in zip(reversed(a_s[1:-1]), unpack_weigths_gen_inv(weights, weights_meta)):\n",
      "            d_new = np.dot(theta[:, 1:].T, d_prev) * (a_i[1:] * (1 - a_i[1:]))\n",
      "            d_s = (d_new, ) + d_s\n",
      "            d_prev = d_new\n",
      "        for d_i, a_i, i in zip(reversed(d_s), reversed(a_s[:-1]), range(len(Deltas) - 1, -1, -1)):\n",
      "            Deltas[i] = Deltas[i] + np.dot(d_i[np.newaxis].T, a_i[np.newaxis])\n",
      "    \n",
      "    thetas_gen = None\n",
      "    if reg_lambda != 0:\n",
      "        thetas_gen = unpack_weigths_gen(weights, weights_meta)\n",
      "    for i in range(len(Deltas)):\n",
      "        Deltas[i] = Deltas[i] / m\n",
      "        if reg_lambda != 0:\n",
      "            Deltas[i][:, 1:] = Deltas[i][:, 1:] + (reg_lambda / m) * thetas_gen.next()[:, 1:]\n",
      "    return pack_weigths(*tuple(Deltas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def minibatch(X, y, batch_size=1):\n",
      "    m = X.shape[0]\n",
      "    batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "    max_batchs = int(math.floor(m / batch_size))\n",
      "    \n",
      "    indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "    X, y = X[indices], y[indices]\n",
      "    while True:\n",
      "        for i in range(max_batchs):\n",
      "            indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "            yield X[indices], y[indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mb_gd(func, func_prime, thetas0, X, y, options=None, args=(), callback=None):\n",
      "    batch_size = options['batch_size']\n",
      "    maxiter = options['maxiter']\n",
      "    learning_rate = options['learning_rate']\n",
      "    tol = options['tol']\n",
      "    disp = options['disp']\n",
      "    thetas = thetas0\n",
      "    diff = 1\n",
      "    prevJ = 1000\n",
      "    i = 0\n",
      "    for i, (_X, _y) in zip(range(1, maxiter + 1), minibatch(X, y, batch_size)):\n",
      "        thetas = thetas - learning_rate * func_prime(thetas, _X, _y, *args)\n",
      "        newJ = float(function(thetas, X, y, *args))\n",
      "        if not np.isnan(newJ) and newJ != float(\"inf\"):\n",
      "            diff = np.abs(newJ - prevJ)\n",
      "            \n",
      "            prevJ = newJ\n",
      "            if diff < tol or i >= maxiter:\n",
      "                break\n",
      "        if disp:\n",
      "            print i, newJ    \n",
      "        if callback is not None:\n",
      "            callback(i, thetas)\n",
      "        i += 1\n",
      "    return thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mb_rmsprop(func, func_prime, thetas0, X, y, options=None, args=(), callback=None):\n",
      "    batch_size = options['batch_size']\n",
      "    maxiter = options['maxiter']\n",
      "    tol = options['tol']\n",
      "    disp = options['disp']\n",
      "    thetas = thetas0\n",
      "    diff = 1\n",
      "    prevJ = 1000\n",
      "    rms = 1\n",
      "    for i, (_X, _y) in zip(range(1, maxiter + 1), minibatch(X, y, batch_size)):\n",
      "        grad = func_prime(thetas, _X, _y, *args)\n",
      "        rms = 0.9 * rms + 0.1 * np.square(grad)\n",
      "        thetas = thetas - np.divide(grad, np.sqrt(rms))\n",
      "        newJ = float(function(thetas, X, y, *args))\n",
      "        if not np.isnan(newJ) and newJ != float(\"inf\"):\n",
      "            diff = np.abs(newJ - prevJ)\n",
      "            prevJ = newJ\n",
      "            if diff < tol or i >= maxiter:\n",
      "                break\n",
      "        if disp:\n",
      "            print i, newJ    \n",
      "        if callback is not None:\n",
      "            callback(i, thetas)\n",
      "    return thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mb_scipy(func, func_prime, thetas0, X, y, options=None, args=(), callback=None):\n",
      "    batch_size = options['batch_size']\n",
      "    maxiter = options['maxiter']\n",
      "    tol = options['tol']\n",
      "    disp = options['disp']\n",
      "    thetas = thetas0\n",
      "    diff = 1\n",
      "    prevJ = 1000\n",
      "    thetas = thetas0\n",
      "    for i, (_X, _y) in zip(range(1, maxiter + 1), minibatch(X, y, batch_size)):\n",
      "        ans = optimize.minimize(func, thetas, jac=func_prime, method=options['mb_opti'], \n",
      "                                args=(_X, _y) + args,\n",
      "                                options={'maxiter': options['mb_opti_maxiter']})\n",
      "        thetas = ans.x\n",
      "        newJ = float(function(thetas, X, y, *args))\n",
      "        if not np.isnan(newJ) and newJ != float(\"inf\"):\n",
      "            diff = np.abs(newJ - prevJ)\n",
      "            \n",
      "            prevJ = newJ\n",
      "            if diff < tol or i >= maxiter:\n",
      "                break\n",
      "        if disp:\n",
      "            print i, newJ\n",
      "        if callback is not None:\n",
      "            callback(i, thetas)\n",
      "    return thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mb_opti(func, func_prime, thetas0, X, y, options=None, args=(), callback=None):\n",
      "    if options['mb_opti'] == 'GD':\n",
      "        return mb_gd(function, function_prime, thetas0, X, y, options=options, args=args, callback=callback)\n",
      "    elif options['mb_opti'] == 'RMSPROP':\n",
      "        return mb_rmsprop(function, function_prime, thetas0, X, y, options=options, args=args, callback=callback)\n",
      "    else:\n",
      "        return mb_scipy(function, function_prime, thetas0, X, y, options=options, args=args, callback=callback)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN(object):\n",
      "    \n",
      "    def __init__(self, hidden_layers, \n",
      "                 opti_method='MB', maxiter=100, tol=0.000001, \n",
      "                 mb_opti='CG', batch_size=0.1, learning_rate=0.3, \n",
      "                 mb_opti_maxiter=10,\n",
      "                 disp=False, opti_callback=None,\n",
      "                 act_func=sigmoid,\n",
      "                 epsilon_init=0.12, random_state=0, \n",
      "                 reg_lambda=0, coef0=None):\n",
      "        self.hidden_layers = hidden_layers\n",
      "        self.opti_method = opti_method\n",
      "        self.maxiter = maxiter\n",
      "        self.tol = tol\n",
      "        self.mb_opti = mb_opti\n",
      "        self.batch_size = batch_size\n",
      "        self.learning_rate = learning_rate\n",
      "        self.mb_opti_maxiter = mb_opti_maxiter\n",
      "        self.disp = disp\n",
      "        self.opti_callback = opti_callback\n",
      "        self.act_func = act_func\n",
      "        self.epsilon_init = epsilon_init\n",
      "        self.random_state = random_state\n",
      "        self.reg_lambda = reg_lambda\n",
      "        self.coef0 = coef0\n",
      "        self.meta_ = None\n",
      "        self.coef_ = None\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return forward(self.coef_, self.meta_, X, self.act_func).argmax(0)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        layers = self.hidden_layers\n",
      "        layers.insert(0, X.shape[1])\n",
      "        layers.insert(len(layers), np.unique(y).shape[0])\n",
      "        weight_metadata = build_network(layers)\n",
      "        self.meta_ = weight_metadata\n",
      "        \n",
      "        np.random.seed(self.random_state)\n",
      "        thetas0 = self.coef0 if self.coef0 is not None else rand_init(weight_metadata, self.epsilon_init)\n",
      "        num_labels = len(np.unique(y))\n",
      "        \n",
      "        options = {}\n",
      "        options['maxiter'] = self.maxiter\n",
      "        options['disp'] = self.disp\n",
      "        _callback = None\n",
      "        \n",
      "        if self.opti_method == 'MB':\n",
      "            if self.opti_callback is not None:\n",
      "                def _callback(i, thetas):\n",
      "                    self.coef_ = thetas\n",
      "                    self.opti_callback(self, i)\n",
      "            \n",
      "            options['mb_opti'] = self.mb_opti\n",
      "            options['batch_size'] = self.batch_size\n",
      "            options['learning_rate'] = self.learning_rate\n",
      "            options['tol'] = self.tol\n",
      "            options['mb_opti_maxiter'] = self.mb_opti_maxiter\n",
      "            ans = mb_opti(function, function_prime, thetas0, X, y, options=options, callback=_callback,\n",
      "                          args=(weight_metadata, num_labels, self.reg_lambda, self.act_func))\n",
      "        else:\n",
      "            if self.opti_callback is not None:\n",
      "                def _callback(thetas):\n",
      "                    self.coef_ = thetas\n",
      "                    self.opti_callback(self, 0)\n",
      "            ans = optimize.minimize(function, thetas0, jac=function_prime, method=self.opti_method, \n",
      "                                    args=(X, y, weight_metadata, num_labels, self.reg_lambda, self.act_func),\n",
      "                                    options=options, callback=_callback)\n",
      "            ans = ans.x\n",
      "        self.coef_ = ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MNIST"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets see how it good it does the using the MINST dataset: 50k rows for training and 10k validations, 748 features.\n",
      "\n",
      "For all the optimization algorithms I am going to use 100 iterations using a batch size of 100. Also I only timed the fitting once because it takes a long time and I am lazy, also the difference between iterations when the times are that long are not that significant.\n",
      "\n",
      "My setup is: Macbook Pro 2.5 GHz Inter Core i5. 16 GB RAM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train = train_set[0], train_set[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_valid, y_valid = valid_set[0], valid_set[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "MB-GD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First let's try using a simple gradient decent, the bad part is that one needs to input [the pesky learning rate](http://arxiv.org/abs/1206.1106)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN(hidden_layers=[25], opti_method='MB', mb_opti='GD', maxiter=100, batch_size=100, learning_rate=0.3, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 1: 131 s per loop\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.68469999999999998"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It took 131 seconds with a 68.46% accuracy, not amazing, let's give him another 100 iterations by setting the `coef0` argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN(hidden_layers=[25], opti_method='MB', mb_opti='GD', maxiter=100, batch_size=100, learning_rate=0.3, coef0=nn.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 1: 135 s per loop\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "0.82030000000000003"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much better, more time will probably give better results."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "RMSPROP"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RMSPROP is an idea from Geoffrey Hinton which is very simple to implement and removes the learning rate. The idea is to reduce the learning rates of each gradient by a RMS of previous batches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN(hidden_layers=[25], opti_method='MB', mb_opti='RMSPROP', maxiter=100, batch_size=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 1: 139 s per loop\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "0.41310000000000002"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not amazing lets try another 100 iterations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN(hidden_layers=[25], opti_method='MB', mb_opti='RMSPROP', maxiter=100, batch_size=100, coef0=nn.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 1: 135 s per loop\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "0.44140000000000001"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hum... Not really good, lets give **more** time"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN(hidden_layers=[25], opti_method='MB', mb_opti='RMSPROP', maxiter=500, batch_size=100, coef0=nn.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 1: 688 s per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: RuntimeWarning: overflow encountered in exp\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "0.69699999999999995"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow, that took a while, and not amazing results to be honest."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "MB-CG"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally my personal favorite is just to use the scipy optimization methods, one has to be careful though because most are for batch optimization. But the Conjugate Gradient and L-BFGS-B works on mini-batches. Personally I prefer CG because gave me better results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN(hidden_layers=[25], opti_method='MB', mb_opti='CG', maxiter=100, batch_size=100, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 1: 218 s per loop\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "0.89280000000000004"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Comparison"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the results in a beautiful pandas DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame([[200, 266, 0.8203], [700, 962, 0.6969], [100, 218, 0.8928]], index=['GD', 'RMSPROP', 'CG'], columns=['# Iter', 'Time [s]', 'Accuracy'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th># Iter</th>\n",
        "      <th>Time [s]</th>\n",
        "      <th>Accuracy</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>GD</th>\n",
        "      <td> 200</td>\n",
        "      <td> 266</td>\n",
        "      <td> 0.8203</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>RMSPROP</th>\n",
        "      <td> 700</td>\n",
        "      <td> 962</td>\n",
        "      <td> 0.6969</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>CG</th>\n",
        "      <td> 100</td>\n",
        "      <td> 218</td>\n",
        "      <td> 0.8928</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "         # Iter  Time [s]  Accuracy\n",
        "GD          200       266    0.8203\n",
        "RMSPROP     700       962    0.6969\n",
        "CG          100       218    0.8928"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In all three benchmarks the winner was CG. Personally I was disapointed about RMSPROP, probally my implementation is wrong, if anyone can spot the mistake I will be happy to know."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The scipy optimization methods are amazing, for basic stuff just use those. Don't have to specify a learning rate and are faster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Something that is not going to be in this post but I want to try is to use mini-batches until some point and then use full-batch to do some fine tuning when mini-batch is not working anymore."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also people suggested to use a lookup table for the sigmoid. I tried but the accuracy I got was really bad, I have to check that code again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As improvements I would love to do is create a general Minibatch optimization and probably use a decorator on RMPSPROP and other optimization options just to follow DRY. Or why not use an existing implemenatation because there are many python implementations of these and more algorithms online, just to name a few I found: [python-rl](https://github.com/amarack/python-rl) and [climin](https://github.com/BRML/climin)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I think I have learned enough about neural networks for my personal satisfaction, so this is probablly the last iteration of my Neural network. I will definitely keep looking at the updates of NN research but not as almost fulltime as I been doing the past weeks. Is an imposible mission to catch up that amount of information in my free time."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having spent almost a month reading about NN my (obvious) conclusion is that researchers are just scratching the surface of what is possible, and the next years are going to be exciting. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is even free [python (gpu) implementation](http://www.cs.toronto.edu/~gdahl/) of a Deep belief net by one person on Geoffrey Hinton group which I havent look but seams promising and hopefully he will release more code. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally I have to admit that I cheated a little bit on the post. Because the neural network supports multiple hidden layers but I only used one!\n",
      "**Why?** because I couldn't find a way to make those network converge without full-batch learning, and the MNIST dataset is too big for that. I have tried on the Iris dataset and is working. Don't trust me? try yourself: the code is on [github](https://github.com/danielfrg/copper)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}