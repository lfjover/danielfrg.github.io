<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Daniel Rodriguez</title><link href="http://danielfrg.github.io/" rel="alternate"></link><link href="http://danielfrg.github.io/feeds/11.atom.xml" rel="self"></link><id>http://danielfrg.github.io/</id><updated>2013-11-23T00:00:00+00:00</updated><entry><title>Harvard Data Science - Fall 2013</title><link href="http://danielfrg.github.io/blog/2013/11/23/harvard-ds/" rel="alternate"></link><updated>2013-11-23T00:00:00+00:00</updated><author><name>Daniel Rodriguez</name></author><id>tag:danielfrg.github.io,2013-11-23:blog/2013/11/23/harvard-ds/</id><summary type="html">&lt;p&gt;A few weeks/months ago I found on HackerNews that Harvard was publishing all their &lt;a href="http://cs109.org/"&gt;Data Science course content&lt;/a&gt; (lectures, videos, labs, homeworks) online. I couldn't miss the opportunity to find what are they teaching, it was a nice experience to see how much I have learned and what else I am missing. Also, when I saw that everything was on python I knew they are doing it right!&lt;/p&gt;
&lt;p&gt;The homeworks are IPython notebooks, the books are what I consider every Data Scientist should read: &lt;a href="http://shop.oreilly.com/product/0636920023784.do"&gt;Python for Data Analysis&lt;/a&gt;, &lt;a href="http://shop.oreilly.com/product/0636920018483.do"&gt;Machine Learning for Hackers&lt;/a&gt;, &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Prologue/Prologue.ipynb"&gt;Probabilistic Programming and Bayesian methods for Hackers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The homeworks were amazing and useful, I haven't seen all the videos (yet) but the ones I did were amazing. Below I do a little review of each homework, the stuff I learned and stuff that I think should be different.&lt;/p&gt;
&lt;p&gt;Initially I wanted to use this opportunity to get into &lt;a href="http://julialang.org/"&gt;Julia&lt;/a&gt;, but the amount of libraries that are available on python and not on Julia was to high for me to drive me back into python (once again).&lt;/p&gt;
&lt;h2&gt;Homework 0&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/harvard-cs109-fall-2013/master/homeworks/HW0/HW0_mysolution.ipynb"&gt;My solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Basic introduction to python and some of it scientific libraries.&lt;/p&gt;
&lt;p&gt;Things I changed:
Use beautifulsoup4 instead of 3, not sure why they decided to go with an deprecated version.&lt;/p&gt;
&lt;h2&gt;Homework 1&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/harvard-cs109-fall-2013/master/homeworks/HW1/HW1_mysolution.ipynb"&gt;My solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The main idea was to scrape data from the web, in my opinion one of the most important things a Data Scientist can do is to grab its own data, it gives a big boost to analysis if the person responsible for the analysis decides upfront what data is needed. Is doesn't need to be perfect at first, but usually helps &lt;strong&gt;a lot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The final problem shows the properties of bootstrapping, a must know. Also some simple problems to get started with matplotlib.&lt;/p&gt;
&lt;p&gt;I knew about &lt;a href="http://www.clips.ua.ac.be/pattern"&gt;pattern&lt;/a&gt; but never had the chance to play with it. I learned that includes a xml/html parser that creates a DOM so one can access the different fields/tags, I usually use &lt;a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/"&gt;beautifulsoup&lt;/a&gt; for html and &lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt; for xml but is nice to have an alternative.&lt;/p&gt;
&lt;p&gt;I had no idea about &lt;a href="http://docs.python.org/2/library/fnmatch.html"&gt;fnmatch&lt;/a&gt;, I always used regular expressions but I think that for simple matching tasks I am going to use fnmatch from now own and hopefully will save me some time.&lt;/p&gt;
&lt;h2&gt;Homework 2&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/harvard-cs109-fall-2013/master/homeworks/HW2/HW2_mysolution.ipynb"&gt;My solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A basic introduction to statistical models by doing a predictions the Obama campaign, very interesting exercise. I felt like Nate Silver and it was kind of the idea, show that is possible to do a simple but useful prediction doing a reasonable amount of work. As with a lot of work with data usually the 80% gets done in 20% of the time but the remaining 20% takes the remaining 80% of the time.&lt;/p&gt;
&lt;p&gt;What I changed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Used scipy.stats for probability calculations instead of doing those myself. DRY.&lt;/li&gt;
&lt;li&gt;I tried the new ggplot for python to do some of the plots faster, worked perfectly&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Homework 3&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/harvard-cs109-fall-2013/master/homeworks/HW3/HW3_mysolution.ipynb"&gt;My solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Interesting problem of making a simple prediction of movies reviews intro two categories (fresh or rotten).&lt;/p&gt;
&lt;p&gt;I liked the problem but I think it would be a good idea to use &lt;a href="http://nltk.org/"&gt;NLTK&lt;/a&gt; instead of hand writing some of the functions and doing a hand made classifier using scikit-learn. I realize that it is &lt;strong&gt;really&lt;/strong&gt; easy to do it using the Vectorizer and Naive Bayes classes and I had never done it before so it was a good learning experience, but an introduction to NLTK would be nice.&lt;/p&gt;
&lt;p&gt;The example is very easy to reproduce in NLTK as NLTKs author &lt;a href="http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier"&gt;showed in his blog&lt;/a&gt;. Is an old post but I was able to reproduce the results a few months ago and I was amazed by the results of a simple sentiment classifier. I believe the course missed an opportunity to teach this.&lt;/p&gt;
&lt;p&gt;I liked the cross-validation and specially the &lt;strong&gt;model calibration&lt;/strong&gt; sections. Also the rotten tomatoes API is a pretty good source of data.&lt;/p&gt;
&lt;h2&gt;Homework 4&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/harvard-cs109-fall-2013/master/homeworks/HW4/HW4_mysolution.ipynb"&gt;My solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For sure the most interesting homework. Not only a nice collaborative filtering problem (with some bayesian analysis) but also an introduction to MapReduce jobs on hadoop using MRJob. I never used MRJob before so it was nice to use it for the first time &lt;strong&gt;but&lt;/strong&gt; I don't think it will replace my love for  &lt;a href="https://github.com/spotify/luigi"&gt;luigi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt; is a nice and small library form the guys at spotify. It comes with hadoop support built in but at the same time can be extended to any kind of data jobs (e.g. postgres or simple text files). Since I found about luigi I have been writing a lot of code as pipelines and I could not be happier.&lt;/p&gt;
&lt;p&gt;MRJob killer feature is EMR support that luigi does not have currently. Another nice feature of MRJob is the first class documentation. Luigi on the other hand is limited to a Readme file and you will have to read the source code to discover some features, but it is worth it.&lt;/p&gt;
&lt;p&gt;I have to say that MRJob is very easy and to &lt;strong&gt;MapReduce only&lt;/strong&gt; jobs is the way to go, I highly recommend it.&lt;/p&gt;
&lt;p&gt;My knowledge about collaborative filtering was (and still is) &lt;strong&gt;very&lt;/strong&gt; limited so I only have to say that I loved to learn about it!&lt;/p&gt;
&lt;h2&gt;Homework 5&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/harvard-cs109-fall-2013/master/homeworks/HW5/HW5_mysolution.ipynb"&gt;My solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Graphs was always that topic that for some reason I haven't dive into yet, but is definitely in my thing to learn list. So I took this opportunity to learn everything I could by watching the lectures and doing the homework.
I also grabbed a copy of &lt;a href="http://graphdatabases.com"&gt;graph databases by O'reilly&lt;/a&gt; from the university library and I am very exited about it, I am a little behind on my reading so it might take a while.&lt;/p&gt;
&lt;p&gt;The homework was about the congress and the relation between them. Nothing fancy but interesting.
Second homework that is about politics, I guess that the data is available but it would be nice to chose a different topic.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The objective of learning new stuff was accomplished. The course gave me perfect introduction to a variety of topics and I have more clear ideas on how to proceed from now own.&lt;/p&gt;
&lt;p&gt;I had a lot of fun doing the homeworks, I learned a lot of stuff I didn't knew before. The statistics sections were a little bit hard for me but that was the idea.  I felt really comfortable with a lot of the tools used so I know I am doing it right. Finally, now I know in what I need to focus more.&lt;/p&gt;
&lt;p&gt;The main conclusion I got is that there is still a lot to learn and the Data Science space keeps changing every day, that is the reason I like it: When I think I am good at something, I realize I am just getting started.&lt;/p&gt;
&lt;p&gt;Thanks to Harvard for publishing all the content online.&lt;/p&gt;</summary><category term="python"></category><category term="data science"></category><category term="NLP"></category><category term="statistics"></category><category term="Bayesian"></category><category term="graphs"></category></entry><entry><title>NLP at scale: Semafor + salt + celery and more</title><link href="http://danielfrg.github.io/blog/2013/11/17/nlp-scale-semafor-salt-celery-more/" rel="alternate"></link><updated>2013-11-17T00:00:00+00:00</updated><author><name>Daniel Rodriguez</name></author><id>tag:danielfrg.github.io,2013-11-17:blog/2013/11/17/nlp-scale-semafor-salt-celery-more/</id><summary type="html">&lt;p&gt;This posts describes the implementation of a simple system to parse web pages using &lt;a href="http://www.ark.cs.cmu.edu/SEMAFOR/"&gt;SEMAFOR&lt;/a&gt; (a SEMantic Analyzer Of Frame Representations) at scale. The system is mainly powered by salt and celery but also uses boto to create worker EC2 instances that parse the documents in parallel and luigi is used to describe the data pipeline in each worker.&lt;/p&gt;
&lt;p&gt;The whole source can be found on github: &lt;a href="https://github.com/danielfrg/semafor-parsing"&gt;danielfrg/semafor-parsing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The main idea is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We are going to have one master box that has: salt master + celery worker that is going to be waiting for tasks&lt;/li&gt;
&lt;li&gt;When the master receives a query (list of urls to parse) is going to spin up N number of minions/workers using boto and is going to provision all of them using salt&lt;/li&gt;
&lt;li&gt;Each minion/worker is going to have SEMAFOR and a celery worker waiting for parsing tasks&lt;/li&gt;
&lt;li&gt;The master creates a set of parsing tasks based on the number of docs and number of instances&lt;/li&gt;
&lt;li&gt;Each minion/worker parses the document first using the readability API to get text content from HTML then tokenizing the text into sentences using NLTK and finally parses the sentences using SEMAFOR&lt;/li&gt;
&lt;li&gt;Each minion/worker uploads the results to S3&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The diagram below tries to describe the system.&lt;/p&gt;
&lt;p&gt;&lt;img alt="System" src="/images/blog/2013/11/semafor-dist/diagram.png" title="System description" /&gt;&lt;/p&gt;
&lt;p&gt;If you dont know what semafor is take a look at the &lt;a href="http://demo.ark.cs.cmu.edu/parse"&gt;example demo&lt;/a&gt; or this is just a basic example:&lt;/p&gt;
&lt;p&gt;Input:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;There&amp;#39;s a Santa Claus!
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;frames&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;target&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Existence&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;spans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;There &amp;#39;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}]},&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;annotationSets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;rank&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mf"&gt;52.10168633235354&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;frameElements&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Entity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;spans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a Santa Claus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}]}]}]}],&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;There&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;#39;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Santa&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Claus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The basic idea is: &lt;em&gt;"Existing"&lt;/em&gt; is related to &lt;em&gt;"There 's"&lt;/em&gt; and &lt;em&gt;"Entity"&lt;/em&gt; is related to &lt;em&gt;"a Santa Claus"&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;How to run this?&lt;/h2&gt;
&lt;p&gt;Very simple, only need to get running the master box. Other options are described in the project README but the easiest way is to use vagrant with the AWS provider, just need to run &lt;code&gt;vagrant up --provider aws&lt;/code&gt; in the master directory, this is going to provision the master box.&lt;/p&gt;
&lt;p&gt;When the box is ready just ssh (&lt;code&gt;vagrant ssh&lt;/code&gt;) and:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit &lt;code&gt;~/semafor/master/app/master/settings.py&lt;/code&gt; OR create/edit &lt;code&gt;~/semafor/master/app/local_settings.py&lt;/code&gt; to look like this.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;S3_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;WHERE THE SEMAFOR FILES WILL BE UPLOADED&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;AWS_ACCESS_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;AWS ACCOUNT KEY&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;AWS_SECRET_KEY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;AAWS ACCOUNT SECRET&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;READABILITY_TOKEN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;READABILITY API TOKEN&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;SALT_MASTER_PUBLIC_ADDRESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;THE IP OF THE MASTER&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;LUIGI_CENTRAL_SCHEDULER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;THE IP OF THE LUIGI SCHEDULER, CAN BE THE SAME SALT MASTER&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Run celery worker: &lt;code&gt;cd ~/semafor/app/master/&lt;/code&gt; and &lt;code&gt;sh start_worker.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Everything is ready now!&lt;/p&gt;
&lt;p&gt;Get some URLS you want to parse and call the celery task &lt;code&gt;semafor_parse(urls, n_instances=1)&lt;/code&gt;. A helper script is provided in &lt;code&gt;semafor/master/test.py&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;How it looks&lt;/h2&gt;
&lt;p&gt;This are some screenshots I took while running it:&lt;/p&gt;
&lt;p&gt;EC2 dashboard when creating 10 instances&lt;/p&gt;
&lt;p&gt;&lt;img alt="EC2 instances" src="/images/blog/2013/11/semafor-dist/instances_ec2.png" title="EC2 instances" /&gt;&lt;/p&gt;
&lt;p&gt;Log on celery when creating 10 instances&lt;/p&gt;
&lt;p&gt;&lt;img alt="Instances log" src="/images/blog/2013/11/semafor-dist/instances_log.png" title="Instances log" /&gt;&lt;/p&gt;
&lt;p&gt;Celery log when the instances are provisioned via salt and the celery workers are connected
&lt;img alt="Celery workers log" src="/images/blog/2013/11/semafor-dist/celery_workers.png" title="Celery workers log" /&gt;&lt;/p&gt;
&lt;p&gt;Luigi UI while running
&lt;img alt="Luigi UI" src="/images/blog/2013/11/semafor-dist/luigi_summary.png" title="Luigi UI" /&gt;&lt;/p&gt;
&lt;p&gt;Luigi dependency graph, really simple for this case.
&lt;img alt="Luigi dependency graph" src="/images/blog/2013/11/semafor-dist/luigi_graph.png" title="Luigi dependency graph" /&gt;&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;I used this opportunity to create a real-life example and keep learning (and loving) salt. At the same time I kept playing with celery and luigi that are 2 libraries that I love mainly because they solve very specific problems and are really easy to use.&lt;/p&gt;
&lt;p&gt;The system took me a few nights and a weekend to build but I am very happy to the results, it was &lt;strong&gt;way&lt;/strong&gt; easier than I originally though and what makes me even happier is that a few months ago I would consider this a 2/3 month project but I did it in less than a week. Definitely using the right tool for the every step is crucial.&lt;/p&gt;
&lt;p&gt;I cannot image to provision the EC2 instances in other way that is not salt, the states are not easy to understand at first but they are really powerful. Also not relying on AMIs was a requirement, they are nice in some cases but &lt;strong&gt;not&lt;/strong&gt; for reproducibility. Salt solves this.&lt;/p&gt;
&lt;p&gt;Celery makes perfect sense when distributing tasks between different servers, and luigi is perfect for developing the data pipeline in every worker: download text + tokenizing + semafor parsing + upload output. Not to mention that develop the distributed tasks are really easy to develop using celery and the data pipeline is super easy to develop using luigi.&lt;/p&gt;
&lt;p&gt;When building this distributed systems using celery I am always thinking: "I should do this on hadoop...” And I love and use hadoop but the reason I did it using other tools is simple: sometimes is just to hard to do it on hadoop... &lt;strong&gt;specially&lt;/strong&gt; when one needs to manage different steps, external tools and intermediate files such as the semafor output.&lt;/p&gt;
&lt;p&gt;On this particular case one advantage is that semafor is written in Java so it should be "easy" to create some Hadoop job on Java, specially with semafor 3 (currently in alpha) that generates a handy .jar with everything in it. But to be honest I have no idea how to write a hadoop job in Java and I have zero interest in learning. I just want to use hadoop as a service, using pig, hive, MRJob or others. I didn’t want to mess with Java or the semafor source code I just wanted to use it and get the output, and in that case celery + luigi made it easy to develop.&lt;/p&gt;
&lt;h2&gt;Improvements to be made&lt;/h2&gt;
&lt;p&gt;I am always looking for opportunities to improve and try new tools, this are some thing I would love to do.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An alternative to readability: I love the product, but I would &lt;strong&gt;love&lt;/strong&gt; to have the same output offline, I have tried &lt;strong&gt;tons&lt;/strong&gt; of boilerplate removal tools, in various languages (python,java,and more) but the best output is always readability.&lt;/li&gt;
&lt;li&gt;Django integration with celery so one has an UI to call the tasks. I read that the integration has improved in &lt;a href="http://docs.celeryproject.org/en/latest/whatsnew-3.1.html#django-supported-out-of-the-box"&gt;celery 3.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Location of the documents: In general should be a good idea to crawl first and the do the semafor parsing. One of my previous posts: &lt;a href="http://danielfrg.github.io/blog/2013/09/11/django-celery-readability-crawler/"&gt;Django + Celery + Readability = Python relevant content crawler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Location of the semafor output: A better place where analytics can be run easily, maybe a graph database. &lt;a href="http://www.neo4j.org/"&gt;Neo4j&lt;/a&gt; should be easy to get running and integrated, but havn't use it.&lt;/li&gt;
&lt;li&gt;Progress bars are always nice, a solution as described &lt;a href="http://docs.celeryproject.org/en/latest/userguide/tasks.html#custom-task-classes"&gt;here&lt;/a&gt; and &lt;a href="https://djangosnippets.org/snippets/2898/"&gt;integration with django&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have any other suggestion, improvement or question leave a comment.
Or did you ran this on the whole &lt;a href="http://www.lsi.upc.edu/~nlp/wikicorpus/"&gt;wikipedia&lt;/a&gt; and found something interesting? Let me know that to.&lt;/p&gt;</summary><category term="python"></category><category term="NLP"></category><category term="semafor"></category><category term="salt"></category><category term="celery"></category><category term="luigi"></category><category term="vagrant"></category></entry><entry><title>Using cloud9 to save computing power and to code from everywhere too</title><link href="http://danielfrg.github.io/blog/2012/11/03/using-cloud9-to-save-computing-power-and-to-code-from-everywhere-too/" rel="alternate"></link><updated>2012-11-03T14:46:00+00:00</updated><author><name>Daniel Rodriguez</name></author><id>tag:danielfrg.github.io,2012-11-03:blog/2012/11/03/using-cloud9-to-save-computing-power-and-to-code-from-everywhere-too/</id><summary type="html">&lt;p&gt;I am taking my second course on &lt;a href="https://www.coursera.org/" title="Coursera"&gt;Coursera&lt;/a&gt; Computational Investing -
Part 1; pretty fun so far. The homework 1 was to find a portfolio of 4
equities with the best (highest) Sharpe ratio from 2011. With a little
bit of coding in python I was able to download all SPY stocks and loop
through out the different combinations of them with different weights to
find the best portfolio. The problem I found was that my PC is to slow
for this task :(&lt;/p&gt;
&lt;p&gt;First I have to said I am not an algorithm expert so the process is
linear, but I was able to optimize a little bit the process by loading
into memory first all the data and some other little stuff that makes
the calculation faster; that is from 1 1/2 minutes initially to 30
seconds combining 5 stocks. And I was able to combine up to 8 stocks on
my PC without any problem but when I try to combine more than 8 well it
was so much for my PC; I even take a shower and the calculation wasn't
over, that is my limit :P&lt;/p&gt;
&lt;p&gt;There are solutions to make this type of calculation on the cloud such
as &lt;a href="http://www.picloud.com/" title="PiCloud"&gt;PiCloud&lt;/a&gt; a service that I am definitely going to try out soon but
I found &lt;a href="https://c9.io/" title="Cloud9"&gt;cloud9&lt;/a&gt; was a solution to this problem too.&lt;/p&gt;
&lt;p&gt;Cloud 9 says they are the Google Docs for code and they really are.
Initially I had my doubts about the online editor mainly because I love
&lt;a href="http://www.sublimetext.com/" title="Sublime Text"&gt;Sublime text&lt;/a&gt;, and yes it is not so powerful as sublime but is
&lt;strong&gt;very&lt;/strong&gt; good. But the features such the Github integration and the
online terminal sold me the service ;I test it and didn't disappoint me.&lt;/p&gt;
&lt;p&gt;I was able with one-click to clone the repository on my Github and run
the code on their infrastructure (powered by &lt;a href="https://openshift.redhat.com/app/" title="OpenShift"&gt;OpenShift&lt;/a&gt;) with no
problems at all. Just need to install the required libraries (&lt;a href="http://numpy.scipy.org/" title="numpy"&gt;numpy&lt;/a&gt;
- running&lt;em&gt;easy_install numpy&lt;/em&gt;) and then it was as simple as run &lt;em&gt;python
portfolio.py&lt;/em&gt; and see the code running much faster than on my PC.&lt;/p&gt;
&lt;p&gt;Also the ability to run the code and edit it from everywhere is amazing.
I was able to connect from my University and keep testing better
portfolios. On the image below I ran 83 iteration of the code in less
than 3 seconds, on my PC takes 20 seconds or more. The max number of
iterations I ran was up to 250k taking up to 8 minutes. And after
modifying the code I was able to push from Cloud9 directly to my Github
and back to my PC. Git makes the flow is very smooth and the terminal
from Cloud9 good enough for that and more.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Django UI" src="/images/blog/2012/11/cloud9/comp-investing-cloud9-google-chrome_001.png" title="Running code in the Cloud9 Terminal" /&gt;&lt;/p&gt;
&lt;p&gt;The service has some problems, sometimes the terminal goes crazy for no
reason (that I could find) and I had to refresh the page, but in general
works great.&lt;/p&gt;
&lt;p&gt;I found this a very easy way to execute code on the cloud saving
computing power and getting faster responses. Next I want to try
&lt;a href="http://www.picloud.com/" title="PiCloud"&gt;PiCloud&lt;/a&gt; and &lt;a href="https://openshift.redhat.com/app/" title="OpenShift"&gt;OpenShift&lt;/a&gt; directly.&lt;/p&gt;
&lt;p&gt;The code for the computational investing is on &lt;a href="https://github.com/dfrodriguez143/comp-investing" title="Computational Investing on Github "&gt;my Github&lt;/a&gt; if anyone
is interested, is going to be updated as the course goes through.&lt;/p&gt;</summary><category term="cloud9"></category><category term="computational investing"></category><category term="coursera"></category><category term="numpy"></category><category term="python"></category></entry></feed>